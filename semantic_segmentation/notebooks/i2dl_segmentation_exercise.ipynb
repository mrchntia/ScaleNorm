{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Semantic Segmentation \n",
    "============\n",
    "\n",
    "In this exercise you are going to work on a computer vision task called semantic segmentation. In comparison to image classification the goal is not to classify an entire image but each of its pixels separately. This implies that the  output of the network is not a single scalar but a segmentation with the same shape as the input image. Think about why you should rather use convolutional than fully-connected layers for this task!\n",
    "\n",
    "Since we already introduced the basics of PyTorch and how to train a NN we leave the model design and architecture as well as the training up to you. We only provide you with the train, validation and test dataset and recommend you to look for inspirational, existing PyTorch implementations. Due to the fairly small size of the segmentation dataset you should not train a model from scratch but consider to (at least partially) finetune weights of an already exsisting model.\n",
    "\n",
    "The infamous  [Fully Convolutional Networks for Semantic Segmentation](https://people.eecs.berkeley.edu/~jonlong/long_shelhamer_fcn.pdf) paper might help you with finding a suitable segmentation model and architecture.\n",
    "\n",
    "<img src=\"https://camo.githubusercontent.com/d10b897e15344334e449104a824aff6c29125dc2/687474703a2f2f63616c76696e2e696e662e65642e61632e756b2f77702d636f6e74656e742f75706c6f6164732f646174612f636f636f7374756666646174617365742f636f636f73747566662d6578616d706c65732e706e67\">"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "is_executing": false
    }
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import os\n",
    "import torch\n",
    "\n",
    "from exercise_code.data_utils import SegmentationData, label_img_to_rgb\n",
    "\n",
    "#torch.set_default_tensor_type('torch.FloatTensor')\n",
    "#set up default cuda device\n",
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "%matplotlib inline\n",
    "plt.rcParams['figure.figsize'] = (10.0, 8.0) # set default size of plots\n",
    "plt.rcParams['image.interpolation'] = 'nearest'\n",
    "plt.rcParams['image.cmap'] = 'gray'\n",
    "\n",
    "# for auto-reloading external modules\n",
    "# see http://stackoverflow.com/questions/1907993/autoreload-of-modules-in-ipython\n",
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "MSRC-v2 Segmentation Dataset\n",
    "================\n",
    "\n",
    "Make yourself familiar with the segmentation dataset and how we implemented the `SegmentationData` class in `exercise_code/data_utils.py`. Furthermore have a look at the labels described in `data/segmentation_data/info.html`. Especially note the label `unlabeled`. Pixels with the label `unlabeled` should neither be considered in your loss nor in the accuracy of your segmentation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data = SegmentationData(image_paths_file='../datasets/segmentation_data/train.txt')\n",
    "val_data = SegmentationData(image_paths_file='../datasets/segmentation_data/val.txt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Train size: %i\" % len(train_data))\n",
    "print(\"Validation size: %i\" % len(val_data))\n",
    "print(\"Img size: \", train_data[0][0].size())\n",
    "print(\"Segmentation size: \", train_data[0][1].size())\n",
    "\n",
    "num_example_imgs = 3\n",
    "plt.figure(figsize=(10, 5 * num_example_imgs))\n",
    "for i, (img, target) in enumerate(train_data[:num_example_imgs]):\n",
    "    # img\n",
    "    plt.subplot(num_example_imgs, 2, i * 2 + 1)\n",
    "    plt.imshow(img.numpy().transpose(1,2,0))\n",
    "    plt.axis('off')\n",
    "    if i == 0:\n",
    "        plt.title(\"Input image\")\n",
    "    \n",
    "    # target\n",
    "    plt.subplot(num_example_imgs, 2, i * 2 + 2)\n",
    "    plt.imshow(label_img_to_rgb(target.numpy()))\n",
    "    plt.axis('off')\n",
    "    if i == 0:\n",
    "        plt.title(\"Target image\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Design and Train your Network\n",
    "==========\n",
    "\n",
    "Implement your network architecture in `exercise_code/classifiers/segmentation_nn.py` and adapt your `Solver` to process segmentation labels. To compensate for the dimension reduction of a typical convolution layer, you should probably include a `nn.Upsample` layer near the end of your network. Also remember to consider finetuning a model instead of training it from scratch.\n",
    "\n",
    "Up until now we only used the default loss function (`nn.CrossEntropyLoss`) of our `Solver` class. However, in order to ignore the `unlabeled` pixels for the computation of our loss, we have to use a customized version of the loss for the initializtation of the `Solver` class. The `ignore_index` argument of the loss can be used to filter the `unlabeled` pixels and computes the loss only over remaining pixels.\n",
    "\n",
    "Step by step:\n",
    "1. Initialize training and validation data loaders.\n",
    "2. Design and initialize a convolutional neural network architecture that has input (N, C, H, W) and output (N, num_classes, H, W) and is based on an already pretrained network.\n",
    "3. Initialize a solver with a loss function that considers the `unlabeled` pixels.\n",
    "4. Adjust the logging of your solver to account for the `unlabeled` pixels.\n",
    "5. Train a segmentation model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "from exercise_code.segmentation_nn import SegmentationNN\n",
    "from exercise_code.solver import Solver\n",
    "import torch.nn.functional as F\n",
    "\n",
    "train_loader = torch.utils.data.DataLoader(train_data, batch_size=10, shuffle=True, num_workers=0)\n",
    "val_loader = torch.utils.data.DataLoader(val_data, batch_size=1, shuffle=False, num_workers=0)\n",
    "\n",
    "model = SegmentationNN()\n",
    "model.to(device)\n",
    "solver = Solver(optim_args={\"lr\": 1e-4, \"weight_decay\": 0.001},\n",
    "                loss_func=torch.nn.CrossEntropyLoss(ignore_index=-1, reduction='mean'))\n",
    "solver.train(model, train_loader, val_loader, log_nth=5, num_epochs=5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Test your Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_data = SegmentationData(image_paths_file='../datasets/segmentation_data_test/test.txt')\n",
    "test_loader = torch.utils.data.DataLoader(test_data,\n",
    "                                          batch_size=1,\n",
    "                                          shuffle=False,\n",
    "                                          num_workers=0)\n",
    "\n",
    "test_scores = []\n",
    "model.eval()\n",
    "for inputs, targets in test_loader:\n",
    "    inputs, targets = inputs.to(device), targets.to(device)\n",
    "    \n",
    "    outputs = model.forward(inputs)\n",
    "    _, preds = torch.max(outputs, 1)\n",
    "    targets_mask = targets >= 0\n",
    "    test_scores.append(np.mean((preds == targets)[targets_mask].data.cpu().numpy()))\n",
    "    \n",
    "model.train()\n",
    "np.mean(test_scores)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Visualization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_example_imgs = 4\n",
    "plt.figure(figsize=(15, 5 * num_example_imgs))\n",
    "for i, (img, target) in enumerate(test_data[:num_example_imgs]):\n",
    "    inputs = img.unsqueeze(0)\n",
    "    inputs = inputs.to(device)\n",
    "    \n",
    "    outputs = model.forward(inputs)\n",
    "    _, preds = torch.max(outputs, 1)\n",
    "    pred = preds[0].data.cpu()\n",
    "    \n",
    "    img, target, pred = img.numpy(), target.numpy(), pred.numpy()\n",
    "    \n",
    "    # img\n",
    "    plt.subplot(num_example_imgs, 3, i * 3 + 1)\n",
    "    plt.axis('off')\n",
    "    plt.imshow(img.transpose(1, 2, 0))\n",
    "    if i == 0:\n",
    "        plt.title(\"Input image\")\n",
    "    \n",
    "    # target\n",
    "    plt.subplot(num_example_imgs, 3, i * 3 + 2)\n",
    "    plt.axis('off')\n",
    "    plt.imshow(label_img_to_rgb(target))\n",
    "    if i == 0:\n",
    "        plt.title(\"Target image\")\n",
    "\n",
    "    # pred\n",
    "    plt.subplot(num_example_imgs, 3, i * 3 + 3)\n",
    "    plt.axis('off')\n",
    "    plt.imshow(label_img_to_rgb(pred))\n",
    "    if i == 0:\n",
    "        plt.title(\"Prediction image\")\n",
    "    \n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Visualize Filter Weights\n",
    "You can visualize the convolutional filters of the very first layer by running the following cell. The kernels should exhibit clear structures of differently oriented edges, corners and circles."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from exercise_code.vis_utils import visualize_grid\n",
    "\n",
    "# first (next) parameter should be convolutional\n",
    "conv_params = next(model.parameters()).data.cpu().numpy()\n",
    "grid = visualize_grid(conv_params.transpose(0, 2, 3, 1))\n",
    "plt.imshow(grid.astype('uint8'))\n",
    "plt.axis('off')\n",
    "plt.gcf().set_size_inches(6, 6)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Save the Model and Submission\n",
    "\n",
    "When you are satisfied with your training, save the model for submission. In order to be eligible for the bonus points you have to achieve an accuracy above __60%__."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "os.makedirs('models', exist_ok=True)\n",
    "model.save(\"models/segmentation_nn.model\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Possible Next Steps\n",
    "\n",
    "1. Implement and integrate a task specific metric such as [Intersection over Union (IoU)](http://www.pyimagesearch.com/2016/11/07/intersection-over-union-iou-for-object-detection/)\n",
    "2. Hyperparameter optimization\n",
    "3. Data augmentation ([PyTorch tutorial](http://pytorch.org/tutorials/beginner/data_loading_tutorial.html))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
